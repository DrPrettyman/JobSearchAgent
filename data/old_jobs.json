[
    {
        "company": "Resend",
        "title": "Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "Email is unlike any other medium in the world. With just an address, anyone can talk to anyone, no matter their location, status, or class. It's democratized access, it's a sacred privilege, and it's worth protecting at all costs.\nResend is building the most accessible email platform for developers. As we've grown to over 15K customers and continue to onboard thousands of new users every day, the pressure of maintaining a safe place for good senders continues to build up.\nAs the first Data Scientist on our Trust & Safety squad, you will wear many investigative hats— identifying abuse patterns, designing purpose build data sets, and developing models that prevent harmful users and enable good senders and scale.\nIn this role, you will…\n\n- Facilitate a novel, zero-friction approach to email sending\n- Investigate the root-causes of anomalies and gang attacks\n- Analyze signal effectiveness: precision, recall, drift, overlap, etc\n- Consult with data engineering to shape purpose-built data pipelines\n- Deeply understand and stay up to date on email abuse vectors and trends\n- Build dashboards and alerts for real-time visibility into behavior and policy violations\n- Build machine learning models to prevent abuse at scale with zero false-positive budget\n\nWho You Are...\n\n- Fluent in SQL and Python (Javascript is a plus)\n- Completed a relevant scientific or mathematical degree\n- Proven ability to productize algorithms for real-time systems\n- You thrive in ambiguity and can build frameworks out of thin air\n- Great communicator that can explain complicated models to non-data folks\n- +2 years of professional experience in relevant role working with massive data sets",
        "questions": [
            {
                "question": "Why do you want to join us?",
                "answer": "The abuse detection problem interests me because it's not static: bad actors adapt, so the signals that work today drift or get gamed. That's closer to the kind of detection problems I worked on in my PhD (identifying early warning signals in noisy systems where the underlying dynamics shift) than typical classification tasks. I like that it demands ongoing investigation rather than train-once-deploy-forever.\n\nAlso, being the first Data Scientist on Trust & Safety means building the frameworks from scratch. I did that at Blink SEO - joined as their first Data Scientist when everything was spreadsheets and built the full ML platform that's now a standalone product. I'm really motivated by this type of zero-to-one work."
            },
            {
                "question": "Tell us about your favorite model you've built.",
                "answer": "At Blink SEO I was really hired just to improve their internal processes by moving the \"download csv, open in excel\" process to databases and dashboards. The software I built as a result increased productivity by 20 times.\n\nOne major time-consuming task for SEO for e-commerce stores is to identify opportunities for new collection pages (which are likely to show in search results). I solved this problem by automating the cleaning, embedding and clustering of Google search terms (weighted for number of searches), then separately clustering the store's products based on embedded product descriptions and metadata, and finally using a weighted centroid matching to identify groups of products which would make a promising new collection. Thanks to the data pipelines and automations I had built, I was able to incorporate this into the software so that the user can click a button to generate these collection opportunities.\n\nThis probably had the largest impact on ROI of all the features because these collection pages are extremely valuable. For our internal team, identifying these opportunities manually (in spreadsheets) was one of the most time-consuming tasks."
            }
        ]
    },
    {
        "company": "Room Price Genie",
        "title": "Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "",
        "questions": [
            {
                "question": "What's Got You Excited About RoomPriceGenie?",
                "answer": "This sounds like a great place to be an employee generally, and also a great place for me because it looks like there is lots of work to be done and I can imagine there are lots of ways to potentially optimise things. I'm really looking for something collaborative because I find the best work gets done when bouncing ideas around, both within the data team and between departments. I also appreciate the mention of \"ownership mentality\" because I tend to get attached to products and want to make things the best they can be, although I have learned not to become precious about individual features: the ownership is of the product, not the code."
            },
            {
                "question": "What's Your Superpower?",
                "answer": "I've always been a deep thinker. This was a great help getting my PhD, and it's also been useful as a developer. I'm continually saying: \"How will this scale?\"; \"How can we generalise this?\"; \"What's the best way to implement this? Here are five more ideas.\". It did take some time to get used to working outside of academia, trying to balance ideas with action, but I've worked a lot on time management since then and I find I enjoy working fast and operating with a CI/CD mindset."
            },
            {
                "question": "What's your proudest moment?",
                "answer": "At Blink SEO I was really hired just to improve their internal processes by moving the \"download csv, open in excel\" process to databases and dashboards. The software I built as a result increased productivity by 20 times. But I built it with scalability in mind from the start and always felt that there's no point inventing a chainsaw just to chop down one tree: you want to market the chainsaw. With the internal tool as an MVP, management were sufficiently impressed that they did decide to seek VC backing and launch my product as a SaaS business: Macaroni Software. That was a proud moment because it really validated my approach."
            }
        ]
    },
    {
        "company": "PEAS",
        "title": "Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "",
        "questions": [
            {
                "question": "Can you describe a time when you realised your code in Python or SQL wasn't the most efficient way of doing something? What changes did you make so it became more efficient?",
                "answer": "When I built the data pipeline at Blink SEO I initially had a python script which streamed data daily from multiple sources into a BigQuery database, and then several SQL table functions to aggregate over different periods (last 28 days, last 90 days, etc.) which powered the various charts and tables in the dashboards I had created. As the product scaled this became very costly so I switched to using views rather than static tables, and also cached daily aggregates on a rolling basis. This required a bit more work but cut database compute costs by 90% and also sped up the user experience."
            },
            {
                "question": "Looking at the job description, which area do you think might be the biggest stretch for you and where you'd like to grow?",
                "answer": "Looking at the job description, I feel I'm familiar with most of the requirements: I have developed and maintained data pipelines before; worked with other developers and stake-holders to ideate and realise solutions; and, working at a small start-up, I have lots of experience in delivering ad-hoc projects alongside working on bigger goals. But I haven't actually worked with DataBricks before, although I have read a lot about it and would be excited to see what I could achieve. It's a really popular platform so gaining experience with it would be fantastic, and I'm sure I could pick it up easily enough given my previous experience."
            },
            {
                "question": "How does this position align with your career aspirations? Feel free to share specific goals or directions you're aiming for.",
                "answer": "I'm always excited to learn and grow in the data sciences and to use my skills to make processes simpler for others, that's why I got into this field. Mostly, however, I would really appreciate the opportunity to work for a charity (I've been specifically looking for roles in the third sector), particularly in education. I'm passionate about the cause and it would be really inspiring to contribute to making a positive difference in the world. As an undergraduate at Edinburgh University I did a lot of voluntary outreach work in local schools and trained to be a teacher before my PhD, then when I was an lecturer (teaching, among other things, a course on Excel For Business) I volunteered to man the \"maths help desk\". Somehow I've managed to end up working in data so it would be wonderful to be able to apply these skills I now have to actually promoting education, particularly for those who have some of the poorest access in the world."
            }
        ]
    },
    {
        "company": "Verif",
        "title": "Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "",
        "questions": [
            {
                "question": "Please describe a project where you focused on developing statistical and machine learning models? Please share the numerical and statistical techniques you applied, and the impact your work had. If applicable, feel free to include any experience related to fraud detection.",
                "answer": "I built the SaaS product Macaroni Software, which incorporated several machine learning and statistical techniques, whilst working for Blink SEO.\n\nOne major time-consuming task for SEO for e-commerce stores is to identify opportunities for new collection pages (which are likely to show in search results). I solved this problem by automating the cleaning, embedding and clustering of Google search terms (weighted for number of searches), then separately clustering the store's products based on embedded product descriptions and metadata, and finally using a weighted centroid matching to identify groups of products which would make a promising new collection. Thanks to the data pipelines and automations I had built, I was able to incorporate this into the software so that the user can click a button to generate these collection opportunities.\n\nThis probably had the largest impact on ROI of all the features because these collection pages are extremely valuable. For our internal team, identifying these opportunities manually (in spreadsheets) was one of the most time-consuming tasks. Management worked out that my software overall increased productivity by twenty times."
            }
        ],
        "cover_letter": "I am writing to apply for the Data Scientist position on Verif's Fraud Engineering team. With over eight years of experience spanning production ML systems and applied research, I am drawn to the challenge of building detection systems that stay ahead of evolving threats.\nAt Blink SEO, I built a full-stack ML platform from the ground up—architecting a Python backend on GCP Compute Engine integrated with BigQuery to process over 50 million data points daily. I developed NLP-powered clustering models, created asynchronous job-queue systems in PostgreSQL, and deployed interactive dashboards that transformed how the team worked. This experience gave me a strong foundation in taking models from prototype to production at scale.\nMy PhD research at the National Physical Laboratory focused on detecting tipping points in complex systems—identifying early warning signals before critical transitions occur. While the domain was geophysical rather than fraud, the core challenge is analogous: recognising subtle patterns in noisy, high-dimensional data that precede significant events. This work sharpened my ability to develop novel detection strategies and validate hypotheses rigorously.\nI bring proficiency in Python, SQL, and the ML stack you've outlined (scikit-learn, Pandas, NumPy, Matplotlib), along with practical experience deploying models in cloud environments. I'm comfortable communicating technical findings to diverse stakeholders, having published in peer-reviewed journals and presented at international conferences.\nI would welcome the opportunity to contribute to Verif's next-generation fraud detection platform and help establish trust online.\nThank you for considering my application."
    },
    {
        "company": "Cochrane",
        "title": "Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "cover_letter": "I am interested in applying for the Data Scientist position. Looking over the job description I'm confident this is a good fit. \nIn my last role as a Data Scientist for Blink SEO I started out by building the database and backend system from scratch, which wasn't exactly what I'd expected but it gave me great experience writing python modules for data ingestion and cleaning; working in SQL; writing wrappers for web-based APIs; and  working closely with the delivery team to understand their processes and how I could automate these and present data usefully in dashboards. As the role progressed I added ML features into the software and built a user-friendly frontend in Retool as the software transitioned from an internal system to a SaaS app (Macaroni Software).\nFor Macaroni Software I was mainly focussed on NLP and clustering algorithms, although I have a broad knowledge of the ML landscape. My Ph.D. research involved detecting tipping points in dynamical systems using multi-dimensional time series data. For this I had to access large datasets using web APIs and I built stochastic models to test hypotheses. \nThank you for your consideration. I look forward to hearing from you and learning more about the specifics of this role. "
    },
    {
        "company": "Tupl",
        "title": "Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "cover_letter": "I am interested in applying for the Python Software Engineer position. Looking over the job description I'm confident this is a good fit. \nIn my last role at Blink SEO I was hired as a “Data Scientist” but started out by building the database and backend system from scratch, which wasn't exactly what I'd expected but it gave me great experience writing efficient python modules; serving our backend from a GCP Compute Engine instance (Debian); writing wrappers for web-based APIs;  and  working closely with the delivery team to understand their processes and how I could automate these. As the role progressed I added ML features into the software and built a user-friendly frontend in Retool as the software transitioned from an internal system to a SaaS app (Macaroni Software).\nI really enjoy writing in great python code, expanding my knowledge of the language an its libraries. \nThank you for your consideration. I look forward to hearing from you and learning more about the specifics of this role. "
    },
    {
        "company": "Cognite",
        "title": "Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "About the job\n\nAbout Cognite\n\nCognite is gearing up for an exciting year of growth in 2026. As we expand our global footprint and scale our impact across industries, we're building our team with talented, forward-thinking individuals who want to make a difference. Even if you don't see a role that matches your experience today, we encourage you to apply and join our talent community. As immediate opportunities open up, our recruiting team will reach out to connect and explore how your skills align with our growing needs. Join us as we continue to innovate and transform how industrial data drives the world forward.\n\nIn the realm of industrial digital transformation, we stand at the forefront, reshaping the future of Oil & Gas, Chemicals, Pharma and other Manufacturing and Energy sectors. Join us in this venture where AI and data meet ingenuity, and together, we forge the path to a smarter, more connected industrial future.\n\nOur values\n\nImpact: Cogniters strive to make an impact in all that they do. We are result-oriented, always asking ourselves.\n\nOwnership: Cogniters embrace a culture of ownership. We go beyond our comfort zones to contribute to the greater good, fostering inclusivity and sharing responsibilities for challenges and success.\n\nRelentless: Cogniters are relentless in their pursuit of innovation. We are determined and deliverable (never ruthless or reckless), facing challenges head-on and viewing setbacks as opportunities for growth.\n\nCognite is a global leader in industrial software and AI with our Industrial DataOps platform, Cognite Data Fusion, at the core. We were awarded the 2022 Technology Innovation Leader for Global Digital Industrial Platforms.\n\nThe Data Science team at Cognite plays a crucial role in building data-driven solutions that empower our clients to make impactful business decisions. As a Data Scientist at Cognite, you will work on challenging data-problems with leaders in the industry. You will be a part of cross-functional teams that include data engineers, solution architects, and project managers. Together, your focus will be on configuring, deploying, and operationalizing digital solutions in sectors such as Oil and Gas, Power & Utilities, and Manufacturing.\n\nAdditionally, you will engage with clients to understand their desired outcomes, lead discovery workshops, and provide insights on potential strategies and obstacles.\n\nThe Data Science team is a part of the EMEA delivery team in our Global Delivery organization. Global Delivery is truly global, with offices in Oslo (Norway), Phoenix & Houston (USA), Tokyo (Japan) and Bengaluru (India). We are a good mix of Project Managers, Data Scientists, Data Engineers, Solution Architects with domain expertise from Oil and Gas, Power & Utilities and Manufacturing.\n\nWe are responsible for delivering successful projects that are flexible and scalable, and foster adoption of our product. In sum, we derive valuable insights from previously hidden data, enabling workers and leaders to transform how heavy asset industries operate to become more efficient and sustainable.\n\nAs a Data Scientist you are responsible for the following, but not limited to:\n\n- Develop and deploy scalable solutions for customer use cases from Oil and Gas, Manufacturing and Power & Utilities industries using Cognite's core capabilities\n- Participate in intensive use case generation workshops with subject matter experts to understand the problem and how it maps to the data\n- Perform data cleansing and exploratory data analysis\n- Implement physical models / machine learning models in Python and deploy to our model hosting environment\n- Design and develop domain specific information and data models\n- Apply GenAI techniques to enhance data-driven decision-making in customer use cases\n- Build dashboards in Grafana, Power BI, Plotly or Dash for visualizing the solutions\n- Utilize and contribute to solution templates and best practices within the Data Science team\n- Mentoring and coaching junior team members is an important part of being a Senior at Cognite\n- Collaborate with data engineers, project managers and solution architects on project deliveries to enable our customers to achieve the full potential of our industrial dataops platform\n- Support customers and partners in conducting data science tasks with Cognite products\n\nWe Believe Most Of These Should Match Your Experience:\n\n- Bachelor/Master of Science in a quantitative field or mechanical, systems, electrical or industrial engineering\n- 2+ years of full-time work experience as a data scientist (preferably within related industry), or data scientist with domain expertise in Oil and Gas or Maintenance or Manufacturing\n- Proficient experience with Python and SQL. Experience with Python in a production setting is a plus\n- Experience with machine learning methods and techniques, statistics and/or optimization\n- Experience with Git\n- Experience working customer faced, with external customers\n- Experience with managed cloud services such as GCP, Azure or AWS is a plus\n- Domain knowledge or a good understanding of the Oil and Gas industry Asset Performance Management - covering i.e production optimization is a big plus\n- Enjoy working in cross-functional teams\n- Able to deliver independently, coach and mentor others internally\n- Enjoy challenges and dare to set ambitious goals that drive innovation\n- Humility to ask for help and enjoy sharing knowledge with others",
        "cover_letter": "At Blink SEO I built an ML platform that increased team productivity by 20x and grew into a standalone SaaS product. I deployed scalable Python systems on GCP, built Plotly dashboards, and worked in a cross-functional team where I engaged directly with stakeholders to understand their workflows. That's the kind of client-focused, production-oriented data science work I want to keep doing, which is why Cognite's approach appeals to me.\nThe platform I built processed 50M+ data points daily from external APIs, with async job queues in PostgreSQL and ML models (NLP clustering, embeddings) deployed to serve real users. I handled everything from data cleansing to dashboard delivery. The part I found most rewarding was iterating with the delivery team: understanding what they actually needed, deploying quickly, and watching features get adopted rather than ignored.\nMy PhD at the National Physical Laboratory involved extracting signals from noisy, high-dimensional time series, which is conceptually similar to deriving insights from operational industrial data. I've published in peer-reviewed journals and presented at international conferences, so I'm comfortable communicating technical work to different audiences.\nI'd be glad to discuss how this experience fits with what Cognite is building.\nThank you for your consideration."
    },
    {
        "company": "Adaptify SEO",
        "title": "AI Engineer",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "About the job\n\nAbout Adaptify SEO\n\nWe see the potential for AI to replace boring, repetitive jobs, and are bringing this in practice by helping marketing agencies deliver SEO fully on autopilot.\n\nAs an AI-first company, we use LLMs everywhere (we developed RAGs before they were called that way). We have a simple and modern stack, ship fast, and work on cutting-edge product features. If you want to join a company and really learn how to make AI/LLM-powered apps that make people's lives easier, Adaptify SEO is the right fit for you.\n\nWe also have collected hundreds of data points, with latent opportunities for data insights, so you could be the person to build powerful analytics and turn data into learnings.\n\nAbout This Role\n\nWe are looking for someone who gets excited about code, loves learning new things, and wants to dive deep into the world of AI-powered products that Adaptify is building. You don't need to be the most experienced developer in the room - we care more about your curiosity, your ability to pick things up quickly, and your passion for building cool stuff.\n\nYour job will be to help make our tech product the best it can be by building exciting new features that push the boundaries of what's possible with AI. You'll be part of a new wave of software development that requires more creativity, experimentation, and rapid context switching between different problems and technologies. We'll fully support you in developing your skills and pursuing your passions. There's lots to do, from crafting clever prompts to building features that feel like magic.\n\nWe are a fully async organisation, we are based in three continents, and you are truly able to set your own working hours. We don't do tickets or stand-ups, we have a work environment that is focused on productivity and no BS with great documentation and the occasional high-impact meeting.\n\nWho You Are\n\nYou're looking to be an early tech hire at a fast-growing and profitable startup, and have an ownership mentality and the ability to come up with innovative ideas autonomously.\nYou are excited about building features that feel like magic and being part of a new wave of software development that requires creativity, experimentation, and rapid context switching.\nYou're a learning machine. Maybe you're newer to the industry, maybe you're self-taught, maybe you're a career changer – what matters is that you absorb new concepts quickly and aren't afraid to dive into unfamiliar territory\nYou're a problem solver who likes digging into stuff and looking at it from different angles, to create powerful new features or fix high-impact bugs\nYou have some professional coding experience – 1-5+ years professionally and you understand the fundamentals and can write clean, functional code. If you have more years of experience we'd love to speak with you too!\nYou're genuinely excited about joining an AI startup and the weird, wonderful challenges that come with building products that weren't possible a few years ago\nYou're comfortable working independently, taking ownership of projects, and aren't shy about asking questions when you need help\nYou have good vibes - you're collaborative, communicative, and bring positive energy to the team\n\nWhat the Job Involves\n\nBuild exciting new features that combine LLMs, prompt engineering, and creative problem-solving in ways that feel magical to our users\nExperiment with cutting-edge AI capabilities and turn wild ideas into working code\nWork with Python FastAPI backend and Firebase to bring innovative features to life\nCollaborate on React + NextJS frontend development to create seamless user experiences\nRapidly switch between different challenges - from crafting the perfect prompt to debugging tricky integrations\n\nTech Stack\n\nPython\nFastAPI\nReact\nNextJS\nFirebase\nLLMs\nPrompt Engineering\nCreative Problem Solving",
        "cover_letter": "I've spent the last three years building an AI-powered SEO platform, so when I saw Adaptify's mission to deliver SEO fully on autopilot, I had to reach out.\nAt Blink SEO I joined as their first data scientist when everything ran on spreadsheets. I built the entire ML platform from scratch: Python backend on GCP, BigQuery processing 50M+ data points daily, NLP-powered clustering that reduced manual keyword grouping from days to minutes, and LLM integrations via Huggingface to generate content recommendations. The platform increased team productivity by 20x and eventually became its own SaaS product, Macaroni Software. I know what it takes to build AI features that feel like magic to end users because I've done exactly that, in the SEO space specifically.\nThe ownership mentality you describe fits how I work. I wasn't hired to build a product, just to improve internal processes. But I saw the potential, built with scalability in mind from day one, and pushed for it to become something bigger. I'm used to rapid context switching, experimenting with new ideas, and shipping fast.\nMy PhD was in applied mathematics, which means I came into industry as a career changer. That transition taught me to pick things up quickly and not be precious about what I don't yet know. I'm comfortable with Python, have worked with React frontends, and have hands-on experience integrating LLMs into production systems.\nI'd love to chat about what you're building and how I could contribute.\nThank you for reading."
    },
    {
        "company": "Syngenta",
        "title": "Geospatial Data Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "# Company Description\n\nSyngenta Group, a global leader in agricultural technology and innovation, employs 60,000 people across more than 100 countries to transform agriculture through tailor-made solutions for farmers, society, and our planet. Our diverse portfolio encompasses seeds, crop protection, nutrition products, agronomic solutions, and digital services, all designed to help farmers produce healthy food, feed, fiber, and fuel while conserving natural resources and protecting the environment. Our mission is to address critical challenges such as climate change and food security through sustainable practices and cutting-edge solutions, while safeguarding the planet's resources.\n\n# Job Description\n\nThe Geospatial Data Scientist will leverage advanced geospatial analytics, machine learning, and remote sensing expertise to transform complex agricultural and earth observation data into actionable insights that drive innovation in Syngenta's Computational Agronomy Department. This role will develop cutting-edge models and algorithms that enable data-informed agricultural decision-making, supporting Syngenta's mission to improve global food security and sustainable farming practices.\n\nWorking within cross-functional teams, the Geospatial Data Scientist will bridge technical expertise with agricultural knowledge to create scalable, data-driven solutions for modern agricultural challenges.\n\n# Accountabilities\n\n- Develop and implement advanced geospatial and machine learning models to analyze agricultural datasets (satellite imagery, drone data, IoT sensors) and extract meaningful patterns.\n- Design, build, and maintain scalable, cloud-enabled large data pipelines for cleaning, transforming, and integrating diverse geospatial data sources.\n- Perform statistical analysis and data mining to uncover spatial and temporal trends that inform agricultural management strategies.\n- Engineer innovative features from remote sensing data to enhance model accuracy and performance.\n- Deliver high-quality, documented code for geospatial data processing using Python and relevant libraries.\n- Translate analytical results into practical recommendations for agronomists, growers, and decision-makers.\n- Stay current with advancements in geospatial technologies, remote sensing, and machine learning to maintain technical leadership.\n- Contribute to technical reports, scientific publications, and presentations to share research outcomes.\n- Collaborate closely with interdisciplinary teams, including agronomists, data scientists, and software engineers.\n\n# Qualifications\n\n**Critical Knowledge & Experience**\n\n- Master's degree in Geographic Information Science, Remote Sensing, Computer Science, Data Science, or a related field with a strong focus on geospatial analysis.\n- 5+ years of experience in satellite and geospatial data analysis and modeling.\n- Proficiency in Python programming, with experience in geospatial libraries such as GeoPandas, Rasterio, and related tools.\n- Expertise in machine learning for earth observation applications (e.g., image classification, object detection, time series analysis).\n- Experience with geospatial foundation models.\n- Experience with version control systems (e.g., Git) and collaborative software development practices.\n- Experience leveraging generative AI tools to optimize workflows, automate tasks, and enhance productivity in geospatial analysis and data science projects.\n\n**Skills**\n\n- Excellent written and verbal communication skills in English.\n- Strong analytical and problem-solving skills, with the ability to explain complex technical concepts to non-technical audiences.\n\n**Nice to have**\n\n- PhD in a relevant field.\n- Familiarity with agronomy concepts and agricultural systems.\n- Expertise in deep learning techniques.\n- Experience with cloud-based geospatial processing and big data technologies (e.g., Google Earth Engine, Spark)."
    },
    {
        "company": "Trimble",
        "title": "Data Engineer / Scientist",
        "date_found": "2025-10-03",
        "applied": true,
        "description": "Are you an enthusiastic and self-motivated Data Engineer / Scientist eager to contribute your expertise to our dynamic and innovative team? Join us in leveraging data to drive meaningful business outcomes and contribute to the success of Trimble's data-driven initiatives.\nWhat You Will Do\nUnleash your potential as a Trimble Data Engineer and Scientist! Working closely with your manager and team, you will leverage your expertise to drive impactful business outcomes, particularly in understanding and improving customer retention for SketchUp. Your path to impactful action starts here:\nDesign, develop, and program methods, processes, and systems to deploy ML models (e.g., logistic regression, XGBoost) to predict churn risk for SketchUp's diverse customer base.\nGenerate actionable insights and solutions by translating complex model outputs into business-friendly insights for targeted retention strategies.\nCollaborate with product and go-to-market teams to engineer impactful features from raw customer data, developing metrics for tracking engagement and activity facilitating better understanding of customer behavior.\nDevelop software programs, algorithms, and automated processes to cleanse, integrate, and evaluate datasets for customer segmentation, ML model training and evaluation.\nExtract meaningful insights from data and effectively communicate findings to optimize churn prediction models using metrics like AUC and lift.\nWhat Skills & Experience You Should Bring\nBachelor's Degree in Computer Science, Data Analytics, Engineering, or a related field. Master's and Ph.D. graduates welcome.\nMinimum of 5 years of experience in data science, data engineering or a related field, with demonstrated expertise in building, evaluating, and deploying machine learning models for predictive analytics, particularly in areas like churn prediction.\nStrong critical thinking abilities and the capacity to work autonomously.\nProficient understanding of key data engineering concepts, such as data lakes, columnar formats, ETL tools, and BI tools, coupled with strong statistical modeling skills for churn analysis and customer segmentation, including cohort analysis and the use of demographic and firmographic data.\nDemonstrated passion and aptitude for problem-solving.\nProficiency in Python & SQL - other languages a benefit.\nEffective collaboration skills within a team environment.\nAbout Trimble\nDedicated to the world's tomorrow, Trimble is a technology company delivering solutions that enable our customers to work in new ways to measure, build, grow and move goods for a better quality of life. Core technologies in positioning, modeling, connectivity and data analytics connect the digital and physical worlds to improve productivity, quality, safety, transparency and sustainability. From purpose-built products and enterprise lifecycle solutions to industry cloud services, Trimble is transforming critical industries such as construction, geospatial, agriculture and transportation to power an interconnected world of work. For more information about Trimble (NASDAQ: TRMB), visit: www.trimble.com\nAbout Your Location\nHybrid location verbiage: Under a flexible work arrangement (hybrid), this resource primarily supports the Architecture, Engineering, Construction and Owners (\"AECO\") Segment and works collaboratively more widely.\nIn a hybrid role, you will work with your manager and team lead to establish a mutually agreeable schedule for your time working in the office based on the position and the business need. Typically hybrid positions are 1-3 days per week, or 20%-60% of the month in the office."
    },
    {
        "company": "ETFbook",
        "title": "Senior ETF Data Analyst/Specialist",
        "date_found": "2025-10-03",
        "applied": true,
        "cover_letter": "I've spent the last three years building a SaaS data platform from scratch, and ETFbook's mission to make ETF data accessible and actionable resonates with what I find most rewarding about this work: turning complex, messy data into something people can actually use.\nAt Blink SEO I built a full-stack ML platform that processed 50M+ data points daily from external APIs, with a Python backend on GCP integrated with BigQuery. I created async job queues in PostgreSQL to handle data imports and analysis tasks, built interactive dashboards in Plotly and JavaScript, and worked closely with delivery teams to translate their workflows into automated pipelines. The platform increased productivity by 20x and eventually became its own SaaS product, Macaroni Software. I've done the end-to-end delivery work you describe: acquiring data, ensuring quality and integrity, building scalable pipelines, and shipping solutions that clients trust.\nI don't have direct ETF industry experience, so I'll be upfront about that. What I do bring is a quantitative foundation (PhD in Mathematics focused on time series analysis), strong Python and SQL skills, and proven ability to work with high-volume data in production systems. My research involved extracting meaningful signals from noisy, multidimensional time series data, which feels relevant to the kind of analytics work ETFbook delivers. I'm a quick learner and genuinely interested in financial markets.\nI'd welcome the chance to discuss how my platform-building experience could contribute to what you're building.\nThank you for reading.",
        "description": "About us:\nETFbook is a Swiss SaaS company providing data, analytics, and custom solutions in the Exchange Traded Funds (ETFs) space. Founded in 2020, we aim to make global ETF data accessible, actionable, and valuable for institutional participants, including ETF issuers, index providers, fund service providers, market makers, exchanges, and buy-side institutions.\nWe develop a platform which offers comprehensive insights through a user-friendly web application and robust APIs, covering areas such as product analysis, fund flow tracking, performance comparisons, and liquidity assessments.\n\nAbout the role:\nYou'll join a highly skilled data analysts team, part of the data department. This team is responsible for shaping data roadmap across three regions incl. EMEA, AMER and APAC, acquiring and defining data and analytics contents, ensuring quality and integrity of the data throughout our solutions available to institutional clients.\nYour primary goal will be to work on key data and analytics contents relevant for institutional clients (buy-and-sell side) for analysing, utilizing, and interpreting data related to global ETFs markets. The goal is to deliver trusted data and analytics solutions to internal stakeholders and predominantly to external clients, enabling data-driven decisions or strategic solutions.\nReporting to the Head of Data, you'll be a part of a collaborative team focused on data contents, data analytics, and data roadmap. This role offers the chance to make a visible impact in a dynamic, fast-evolving field.\n\nResponsibilities:\nDefine key ETF data and analytics contents relevant for institutional clients (buy-and-sell side) for analysing, utilizing, and interpreting data related to global ETFs markets, including ETF markets in EMEA, AMER and APAC.\nWork on shaping data and analytics contents available through ETFbook platform and other distributing methods (Web-Application and APIs) to meet institutional clients (buy-and-sell side) data and analytics needs.\nWork closely with Business, Tech and Product teams on unified, efficient, and collaborative approach to ensure great depth of data and breadth of analytics within ETFbook solutions.\nWork and contribute to end-to-end delivery of ETF data contents, including reference data, dynamic data, holdings, performance, liquidity, and many others.\nWork and contribute to end-to-end delivery of large datasets from multiple sources, ensuring they are clean and structured and work with data engineers on ensuring automation and continuity, by leveraging latest technologies in Big Data and AI/ML.\nMaintain and improve data quality, accuracy, and integrity across various ETFbook solutions (Web-Application and APIs).\nConsult with internal stakeholders and external clients on data and analytics contents and solutions to ensure ETFbook competitive edge and growth in the ETF data space.\n\nRequirements:\n7+ years of experience in the ETF/indexing industry within data, analytics, portfolio management, risk, or research, preferably gained at an ETF issuer or an index provider.\nDesired experience in working with high-frequency financial data, such as tick-level market data and exchange feeds.\nPreferred Master degree in finance, data analysis, data science or related field.\nStrong proficiency in Python/R/SQL and experience in Data Visualisation techniques. Experience in Databricks preferred.\nExperience in project delivery across cross-functional teams.\nExcellent communication skills, with ability to translate complex concept into clear messages for internal (e.g. tech teams) and external purposes (e.g. clients, research)."
    },
    {
        "company": "Moniepoint",
        "title": "Senior Data Scientist (Credit)",
        "date_found": "2026-01-05",
        "applied": true,
        "link": "https://moniepoint.com/careers/roles/4736481101",
        "location": "Remote, Spain",
        "description": "Who we are\n\nMoniepoint is a global fintech building modern financial services for millions of people and businesses across high-growth markets. We provide payments, banking, credit, and financial management tools - reliable products that people and businesses use every day to run their lives, grow their companies, and move money safely.\n\nOur mission is simple: to enable financial happiness for every African, everywhere. And this is day one. We've grown rapidly in Nigeria and the UK, and we're now expanding our product, engineering, and analytics teams. Our work ranges from building financial infrastructure to designing intuitive customer experiences for emerging markets - solving real, meaningful problems at scale.\n\nWe onboard over a million new customers each month, process 100s of billions of dollars in payments annually, and support tens of millions of users across our ecosystem. Our teams operate in a fast-paced, high-impact environment alongside international leaders from companies like Glovo, Bolt, Monzo, Klarna, Checkout.com, and Tide. If you want to build our data science function from the ground up, work with one of Africa's highest-scale fintech data sets, and ship products that influence tens of millions of users, this is an exceptional time to join.\n\nAbout the role:\n\nAt Moniepoint, data is at the core of everything we do. We are a customer-centric company, and your work will enable our teams to make informed, data-driven decisions that directly impact the success of our business. We are seeking a data-driven, impact-oriented Senior Data Scientist to power Moniepoint's consumer lending decisions at scale. You will build the models, experiments, and insights that shape how we acquire, underwrite, and retain millions of customers across Nigeria and Africa.\n\nThis role is pivotal in driving our pricing, credit limit modelling, and credit model deployment strategy in a growing business with unparalleled opportunities to give access to millions of customers.\nYou'll collaborate with our amazing Consumer Credit team and drive how we think about risk ranking, product offering, and data infrastructure to create the best personalised offers for our customers. You'll be responsible for building our first suite of credit risk models and help drive a culture of how we should think about machine learning vs more traditional methods - bringing fresh thinking to inspire both the team and our customers.\n\nSitting at the intersection of data science, credit risk, and product, you'll translate complex data into actionable decisions that drive growth with healthy portfolio performance.\n\nWhat You'll Do\n\n- Develop and deploy credit scoring, affordability, and behavioural models to support underwriting, pricing, and collections\n- Design and run experiments to optimise approval rates, loss rates, and profitability\n- Partner with product squads to embed decision logic into real-time systems\n- Ensure data quality, compliance, and ethical use of models across all decisioning processes\n- Mentor product squads on best practices in experimentation and data-driven decision making\n- Provide models to optimise outcomes in collections, churn management and user retention\n\nWe would love to hear from you if…\n\n- You're comfortable with Statistics - and have a Degree or qualifications in a quantitative field (Statistics, Mathematics, Engineering or similar)\n- You have +5 years of experience in data science, decision science, or risk analytics within financial services\n- Working knowledge of credit risk, consumer lending, and regulatory considerations\n- Proficiency in SQL and at least one modelling/programming language (Python, R)\n- Experience with A/B testing, machine learning, collections modelling and churn management\n- Ability to translate complex analyses into clear recommendations for business stakeholders\n- High ownership mindset and comfort working in fast-paced, cross-functional teams\n\nWhat we can offer you\n\n- Culture: We put our people first and prioritize the well-being of every team member. We've built a company where all opinions carry weight and where all voices are heard. We value and respect each other and always look out for one another. Above all, we are human.\n- Learning: We have a learning and development-focused environment with an emphasis on knowledge sharing, training, and regular internal technical talks.\n- Compensation: You'll receive an attractive salary, pension, health insurance, monthly bonuses, plus other benefits",
        "fit_notes": "ML production experience and ability to build from scratch align perfectly. Focus is on African markets - check timezone compatibility.",
        "cover_letter_topics": [
            {
                "topic": "Build data science function from the ground up; first suite of credit risk models; greenfield opportunity",
                "relevant_experience": "Joined Blink SEO as first data scientist when everything was spreadsheets; built entire ML platform from scratch; internal tool grew into standalone SaaS product"
            },
            {
                "topic": "Scale: million new customers/month, 100s of billions in payments, tens of millions of users, Africa's highest-scale fintech data sets",
                "relevant_experience": "Built systems processing 50M+ data points daily; async job queues handling high-volume workloads; designed for scale from the start"
            },
            {
                "topic": "Team of international leaders from Glovo, Bolt, Monzo, Klarna, Checkout.com, Tide; fast-paced high-impact environment",
                "relevant_experience": "Three years at small startup shipping production features; comfortable with ambiguity and fast iteration"
            },
            {
                "topic": "Drive culture of ML vs traditional methods; bringing fresh thinking to inspire team and customers",
                "relevant_experience": "PhD research background gives perspective on when statistical rigour matters vs when to move fast; pushed for ML approaches at Blink that became core product features"
            },
            {
                "topic": "Intersection of data science, credit risk, and product; translate complex data into actionable decisions",
                "relevant_experience": "Worked at intersection of data and delivery team; built features based on what users actually needed; dashboards that got adopted rather than ignored"
            },
            {
                "topic": "Ownership culture: all opinions carry weight, all voices heard; high ownership mindset",
                "relevant_experience": "Owned entire ML platform; pushed for internal tool to become commercial product; took initiative beyond original job scope"
            },
            {
                "topic": "Mission: enable financial happiness for every African; solving real meaningful problems at scale in emerging markets",
                "relevant_experience": "Motivated by impact; previously sought third-sector roles (PEAS application); PhD research on climate systems was mission-driven"
            },
            {
                "topic": "Quantitative foundation (Statistics, Mathematics); Python and SQL proficiency",
                "relevant_experience": "PhD Mathematics; 10+ years Python; BigQuery and PostgreSQL in production"
            }
        ],
        "cover_letter": "At Blink SEO I built an ML platform from the ground up that increased team productivity by 20x. The role started as \"improve our spreadsheet workflows\" and ended as Macaroni Software, a standalone SaaS product. That zero-to-one trajectory is what draws me to this role.\nI spent three years at Blink as their first data scientist and built everything: Python backend on GCP, BigQuery processing 50M+ data points daily, PostgreSQL job queues for async workloads. The ML work included sklearn clustering weighted by commercial metrics, text embeddings for semantic matching, and scoring systems that ranked opportunities by predicted value. I built a job queue in PostgreSQL to run these models asynchronously so users got results without waiting. The job meant shaping how models fit into the product and working directly with the delivery team until features actually got used. The ownership culture you describe fits how I work.\nMy PhD focused on detecting regime changes in noisy time series: identifying when a system is about to shift state. The core technique was spectral analysis of variance and autocorrelation as early warning signals, which is conceptually similar to tracking behavioural indicators that precede churn or default. I've published on these methods in peer-reviewed journals.\nI'm based in Spain and the remote setup works well. Happy to discuss."
    },
    {
        "company": "Clarity AI",
        "title": "Senior Data Scientist",
        "date_found": "2026-01-05",
        "applied": true,
        "link": "https://job-boards.eu.greenhouse.io/clarityai/jobs/4739620101",
        "location": "Madrid (Remote/Hybrid)",
        "description": "Join 20+ international Data Scientists improving ML systems for sustainability data. Requires 3+ years DS experience, LLMs, agentic workflows, prompt engineering, RAG, vector databases. Production Python, English C1+. Backed by BlackRock, SoftBank, Deutsche Börse.",
        "fit_notes": "Excellent match - LLM/RAG experience from Blink, production ML systems, Python. Climate/sustainability focus aligns with research background. Remote Spain option ideal.",
        "cover_letter_topics": [
            {
                "topic": "Python data science ecosystem (Pandas, Matplotlib, Scikit-Learn); deploying end-to-end ML models",
                "relevant_experience": "Built full ML platform at Blink processing 50M+ data points daily; NLP clustering with sklearn, text embeddings, DBSCAN; Plotly/Matplotlib dashboards"
            },
            {
                "topic": "LLMs, prompt engineering, RAG, vector databases",
                "relevant_experience": "Integrated HuggingFace models for content generation; built prompt engineering workflows; deployed to production on GCP"
            },
            {
                "topic": "Sustainability sector; mission of 'bringing societal impact to markets'",
                "relevant_experience": "MRes and PhD part of EPSRC Mathematics of Planet Earth programme (climate/sustainability focus); PhD on tipping point detection in climate systems; published in Environmental Research Letters"
            },
            {
                "topic": "Team of 20+ international Data Scientists from top academic institutions",
                "relevant_experience": "PhD (Reading), MRes (Imperial College); 3 peer-reviewed publications; presented at international conferences"
            },
            {
                "topic": "Strong software engineering foundations (testing, code review, design); scalable ML production environments",
                "relevant_experience": "Built SaaS product from scratch; PostgreSQL job queues for async processing; GCP Compute Engine deployment; system scaled to handle enterprise clients"
            },
            {
                "topic": "Startup experience",
                "relevant_experience": "Joined Blink SEO as first data scientist; built internal tool that became standalone SaaS product (Macaroni Software)"
            },
            {
                "topic": "Translate business objectives into technical requirements; communicate to non-technical stakeholders",
                "relevant_experience": "Worked directly with delivery team to understand workflows; built dashboards that got adopted; 20x productivity increase"
            }
        ],
        "cover_letter": "I've spent the last three years building production ML systems. At Blink SEO I built a platform that processed 50M+ data points daily: sklearn clustering for keyword grouping, text embeddings for semantic matching, HuggingFace integrations for content generation. The system ran on GCP with PostgreSQL job queues handling async workloads. It started as an internal tool and grew into Macaroni Software, a standalone SaaS product.\nThe sustainability focus is what drew me to Clarity AI. My MRes and PhD were part of the EPSRC Mathematics of Planet Earth programme, which trains researchers specifically for climate and environmental work. My PhD focused on tipping point detection in climate systems. I spent five years at the National Physical Laboratory analysing meteorological time series, published in Environmental Research Letters, and presented at international conferences.\nI saw you're looking for someone who can work in a collaborative, scalable ML production environment. At Blink I worked closely with the delivery team to translate their workflows into automated pipelines and dashboards. The features that got built were the ones they actually used, which meant constant iteration based on feedback. That back-and-forth is where I do my best work.\nI'm based in Spain and interested in remote or hybrid. Happy to discuss further."
    },
    {
        "company": "Tidio",
        "title": "Senior Machine Learning Engineer (NLP/LLMs)",
        "date_found": "2026-01-05",
        "applied": true,
        "link": "https://tidiocareer.recruitee.com/o/senior-machine-learning-engineer-nlpllms",
        "location": "Remote-first (Poland-based, offices in Poland or coworking)",
        "description": "8-person ML team building Lyro, an advanced RAG AI Agent on Claude/Gemini. Projects include: multi-agent workflow systems, agentic product intelligence (search/recommendation), knowledge base management AI, context optimization engine. 350M unique users/month view widget. Received 10M PLN grant for fine-tuning open-source models. Requirements: 4+ years NLP/deep learning in production, LLM experience (Claude/GPT/Gemini), Python expert, PyTorch/HuggingFace, FastAPI/Flask, strong software engineering. Salary: 23-33k PLN net B2B or 19-27k gross employed. Benefits: 26 days off, MacBook Pro, private medical, mental health program, English classes budget.",
        "fit_notes": "Excellent match: NLP/LLM production experience, Python expert, HuggingFace integrations from Blink, FastAPI. Projects align well (RAG, multi-agent, recommendations). Concern: Poland-based but remote-first. Need to verify if they hire in Spain/EU. Salary in PLN suggests Poland contract preference."
    },
    {
        "company": "CI&T",
        "title": "Data Scientist",
        "date_found": "2026-01-05",
        "link": "https://jobs.lever.co/ciandt",
        "location": "Remote (global company, 8000+ employees, 30 years history)",
        "description": "Tech transformation specialists using AI. Work with world's largest brands on ML/analytics. Responsibilities: translate business challenges to DS problems, build ML models (regression, classification, clustering, forecasting), A/B testing, deliver PoCs and production models in Agile teams, data storytelling, model explainability. Requirements: Python (pandas, sklearn, statsmodels), SQL, cloud platforms (AWS/Azure/GCP), Databricks/BigQuery/SageMaker, Git, model deployment, consulting mindset, advanced English. Nice to have: Explainable AI, graph analytics, recommender systems, NLP, Plotly/Tableau, MLOps (MLflow, DVC).",
        "fit_notes": "Strong match: Python/SQL/GCP/BigQuery experience, ML models in production, clustering/forecasting, Plotly dashboards, consulting mindset from working with delivery team. Nice-to-haves align well (NLP, recommender systems, Plotly). Need to verify Spain/EU remote eligibility."
    },
    {
        "company": "VML MAP",
        "title": "Data Scientist",
        "date_found": "2026-01-05",
        "applied": true,
        "link": "https://job-boards.greenhouse.io/map/jobs/8350011002#app",
        "location": "Málaga, Spain",
        "description": "Data Scientist in Data & AI practice. Develop predictive models (propensity, churn, lookalike), recommendation systems, customer analytics (segmentation, LTV, behavioral analysis). Production-aware ML, working with Data Engineers on deployment. Python, SQL, cloud ML platforms (GCP Vertex AI, Salesforce Einstein). Agency working with world-renowned brands on marketing and customer experience. 1000+ specialists from 55+ nationalities.",
        "fit_notes": "Strong match: Python/SQL, production ML experience, propensity/clustering models from Blink. Local Málaga office. Marketing/CRM focus similar to SEO background. Cross-functional collaboration experience aligns well.",
        "cover_letter_topics": [
            {
                "topic": "Production-aware mindset: moving models from experimentation to production, working with Data Engineers on deployment, scoring pipelines, orchestration",
                "relevant_experience": "Built full production ML system at Blink from scratch; PostgreSQL job queues for async scoring; GCP Compute Engine deployment; system scaled to handle enterprise clients with 50M+ data points daily"
            },
            {
                "topic": "Propensity models, lookalike/similarity models, customer segmentation, recommendation systems",
                "relevant_experience": "Built clustering models for keyword grouping (weighted by search volume, similar to propensity scoring); product-keyword matching using centroid similarity (lookalike model); HuggingFace integrations for content recommendations"
            },
            {
                "topic": "Python and SQL proficiency; cloud ML platforms (GCP)",
                "relevant_experience": "10+ years Python; BigQuery processing 50M+ data points daily; GCP Compute Engine backend; sklearn, Pandas, NLTK"
            },
            {
                "topic": "Solid statistical foundation; statistical rigor in methodologies",
                "relevant_experience": "PhD Mathematics; published in peer-reviewed journals (ERL, Chaos, EPL); statistical modeling for tipping point detection"
            },
            {
                "topic": "Translate business challenges into data-driven solutions; advising clients; communicating complex findings clearly",
                "relevant_experience": "Worked directly with delivery team to understand workflows; built dashboards they actually used; 20x productivity increase; features iterated based on feedback"
            },
            {
                "topic": "Marketing domain; customer experience outcomes",
                "relevant_experience": "3 years in SEO/marketing tech at Blink; built platform for e-commerce clients; familiar with marketing metrics and customer analytics"
            },
            {
                "topic": "Cross-functional collaboration with engineering, strategy, and client teams",
                "relevant_experience": "Worked in small cross-functional team; collaborated with delivery team and stakeholders; presented at international conferences"
            },
            {
                "topic": "Not just a notebook data scientist; good engineering practices (Git, clean code)",
                "relevant_experience": "Built maintainable SaaS product; version control; async job queues; code that scales"
            }
        ],
        "cover_letter": "I've been building production ML systems in marketing tech for the last three years. At Blink SEO I created a platform that processed 50M+ data points daily through a Python backend on GCP, with PostgreSQL job queues handling async scoring and analysis. The system increased team productivity by 20x and eventually became its own SaaS product.\nThe similarity and clustering work I did transfers well. I built centroid matching to connect products with high-value search terms, clustering weighted by search volume, and LLM integrations for content recommendations. The domain was SEO rather than CRM, but the techniques overlap: embedding entities, scoring by weighted attributes, finding matches using similarity metrics.\nWorking at an agency meant collaborating with the delivery team on what to build and how. I translated their workflows into automated pipelines, built dashboards they actually used, and iterated based on feedback. The code had to work reliably for enterprise clients, so I focused on maintainable systems: version control, async processing, clean interfaces between components.\nMy PhD in Mathematics at the National Physical Laboratory focused on detecting patterns in noisy time series data. I published in peer-reviewed journals and presented at international conferences, so I'm comfortable explaining technical work to different audiences.\nI'm based in Málaga and interested in learning more about the role."
    },
    {
        "company": "Management Solutions",
        "title": "Data Science Consultant",
        "date_found": "2026-01-05",
        "applied": true,
        "link": "https://recruiting.managementsolutions.com/VerOfertas.aspx?num=271",
        "location": "Málaga, Spain",
        "description": "Consulting role focused on data mining, predictive modeling, and analytical support to major organizations. Requires background in Mathematics, Physics, Statistics, Econometrics, or related quantitative fields. Knowledge of modeling techniques including logit, GLM, time series, decision trees, random forests. Proficiency in SAS, R, Python, or Matlab. Role offers exposure to high-profile projects with leading companies, approximately 600 hours of training during first two years, merit-based advancement and partnership model.",
        "fit_notes": "Local Málaga office. Technical skills match well (Python, time series, modeling). However, role targets 'recent graduates or final-year students' so may be too junior. Worth checking if they have senior positions.",
        "cover_letter_topics": [
            {
                "topic": "Background in Mathematics, Physics, Statistics, Econometrics, or related quantitative fields",
                "relevant_experience": "PhD Mathematics (Reading), MRes Mathematics (Imperial College), MA Mathematics (Edinburgh, First Class). Published in peer-reviewed journals on statistical methods."
            },
            {
                "topic": "Knowledge of modeling techniques: logit, GLM, time series, decision trees, random forests",
                "relevant_experience": "PhD focused on time series analysis and tipping point detection. Built clustering models (sklearn) and recommendation systems at Blink. Familiar with tree-based methods and statistical modeling."
            },
            {
                "topic": "Proficiency in Python or Matlab",
                "relevant_experience": "10+ years Python; built full ML platform processing 50M+ data points daily. PhD research used MATLAB for numerical simulations."
            },
            {
                "topic": "Data mining, predictive modeling, analytical support to major organizations",
                "relevant_experience": "Built predictive models and automated analytics at Blink for e-commerce clients. Created dashboards and pipelines that increased productivity by 20x."
            },
            {
                "topic": "Consulting/client-facing work with leading companies",
                "relevant_experience": "Worked directly with delivery team and stakeholders to understand needs. Built solutions for enterprise clients. Comfortable presenting technical work to different audiences."
            }
        ],
        "cover_letter": "The consulting model at Management Solutions interests me: at Blink I built a platform from scratch that processed 50M+ data points daily. Python backend on GCP, sklearn for clustering and predictive models, PostgreSQL for async job queues. The work involved translating client needs into automated analytics. I'd meet with the delivery team, understand their workflows, then build pipelines and dashboards that actually got used. The system increased productivity by 20x and eventually became a SaaS product.\nSince leaving Blink I've taken on freelance projects: model deployment, dashboard creation in Tableau, pipeline automation with dbt and Databricks. I enjoy the variety and the direct client relationships.\nMy PhD in Mathematics focused on time series analysis and tipping point detection: I used MATLAB for numerical simulations and published in peer-reviewed journals. That gave me strong foundations in statistical modeling, and presenting at international conferences taught me to communicate technical work clearly.\nThe exposure to high-profile projects and the partnership model both appeal to me.\nI'm based in Málaga and available to talk."
    },
    {
        "company": "agap2 Spain",
        "title": "Data Scientist",
        "date_found": "2026-01-06",
        "link": "https://agap2.com/spain/en/jobs/data-scientist-remote/",
        "location": "100% Remote (Spain), indefinite contract",
        "description": "Senior Data Scientist: definition and deployment of predictive algorithms. Tech: AWS (Terraform), Spark, Python, Apache Airflow, Jenkins CI/CD. Work with structured/unstructured data, prepare datasets for model training, implement and improve forecasting models, collaborate with business for data-driven decisions. Requirements: 4+ years Python or OOP language, data science project experience, AWS experience a plus. Benefits: indefinite contract from day one, 100% remote, flexible hours, continuous training/certifications, performance reviews, flexible remuneration, restaurant ticket, medical insurance.",
        "fit_notes": "Excellent location fit: 100% remote Spain with indefinite contract. Tech stack: Python matches perfectly, Airflow/Spark/AWS are learnable (have GCP experience). Forecasting models align with time series PhD background. Consulting variety appeals.",
        "cover_letter_topics": [
            {
                "topic": "Python proficiency (4+ years required)",
                "relevant_experience": "10+ years Python; built full ML platform from scratch; sklearn, Pandas, NLTK, Matplotlib, Plotly"
            },
            {
                "topic": "Definition of innovative solutions (not just implementing, but architecting from scratch)",
                "relevant_experience": "Defined entire ML platform architecture at Blink when nothing existed; identified automation opportunities; pushed for internal tool to become commercial product"
            },
            {
                "topic": "Forecasting models; predictive algorithms",
                "relevant_experience": "PhD focused on time series forecasting (tipping point detection using spectral analysis); sklearn clustering and scoring systems in production at Blink"
            },
            {
                "topic": "Full-stack comfort: cloud infrastructure, orchestration, CI/CD (AWS, Terraform, Airflow, Jenkins mentioned)",
                "relevant_experience": "GCP Compute Engine + BigQuery at Blink; PostgreSQL job queues for orchestration; continuous deployment; AWS in freelance work; comfortable across the stack"
            },
            {
                "topic": "European engineering consultancy; challenging projects in dynamic environment; varied clients",
                "relevant_experience": "Freelancing since Jan 2025 across different clients and stacks (dbt, Databricks, Tableau, AWS); three years at fast-moving startup; comfortable adapting quickly"
            },
            {
                "topic": "Work with structured/unstructured data; prepare datasets for model training",
                "relevant_experience": "Built ETL pipelines at Blink; NLP work with unstructured text (keyword clustering, embeddings); data cleaning from external APIs"
            },
            {
                "topic": "Collaborate with business for data-driven decisions",
                "relevant_experience": "Worked directly with delivery team; translated workflows into automated pipelines; dashboards that got adopted; 20x productivity increase"
            },
            {
                "topic": "Career development culture; continuous learning emphasis",
                "relevant_experience": "PhD shows deep learning commitment; transitioned academia to industry; continuously expanding stack (recent: dbt, Databricks, Tableau)"
            }
        ],
        "cover_letter": "At Blink SEO I defined and built a full ML platform from scratch. There was no existing system: I architected the Python backend on GCP, set up BigQuery to handle 50M+ data points daily, and created PostgreSQL job queues to orchestrate async workloads. The sklearn clustering and NLP features I built increased team productivity by 20x, and the platform eventually became a standalone SaaS product.\nThe role meant owning the full stack: data preparation, model development, infrastructure, deployment. I worked directly with the delivery team to understand what they needed, then built pipelines and dashboards they actually used. Continuous deployment based on feedback.\nSince January 2025 I've been consulting, taking on projects across different clients and stacks: pipeline automation with dbt and Databricks, dashboards in Tableau, database setup in AWS. The variety suits me and I've gotten comfortable adapting quickly to new environments.\nMy PhD focused on time series forecasting: detecting tipping points in noisy systems using spectral analysis. The domain was geophysical but the statistical foundations transfer directly.\nI'm based in Spain. Happy to discuss."
    },
    {
        "company": "Wikimedia Foundation",
        "title": "Senior Data Scientist",
        "date_found": "2026-01-06",
        "link": "https://job-boards.greenhouse.io/wikimedia/jobs/7493811",
        "location": "Remote (Spain eligible)",
        "description": "Community Growth team supporting global Wikimedia volunteers through grantmaking and capacity development. Build data systems for grant-funded activities enabling standardization and connection with primary data pipelines. Create/maintain internal dashboards (Apache Superset, Google Looker Studio) tracking program outcomes. Extract actionable insights to inform grantmaking decisions. Design data processes, build data frames, connect to pipelines for storage/retrieval, basic aggregation and ETL. Requirements: Bachelor's + 5 years OR Master's + 3 years in data analysis/science. SQL and Python proficiency, API integration in Python (auth, pagination, JSON), large-scale data tools (Hadoop, Hive, Presto, Spark), data visualization tools, Google Suite/advanced spreadsheets. Desired: statistical training, nonprofit/NGO experience, cross-cultural awareness, open-source tools. Salary: $106,227-$167,858 (US), adjusted by country. Remote-first, 40+ countries including Spain.",
        "fit_notes": "Excellent fit: Remote-first with Spain eligibility. Strong match for Python/SQL skills, API integrations, data pipeline experience, and dashboard building. Mission-driven nonprofit aligns with interest in impact-focused work. NGO/nonprofit experience is 'nice to have' not required.",
        "full_description": true
    },
    {
        "company": "Typeform",
        "title": "Sr. Machine Learning Engineer",
        "date_found": "2026-01-06",
        "link": "https://job-boards.greenhouse.io/typeform/jobs/7446908",
        "location": "Remote (Spain, Germany, UK, Ireland, Netherlands, Portugal)",
        "description": "Data & Insights team building ML capabilities for conversational data collection (150K+ businesses, 500M responses/year). Design/deploy scalable ML systems using Docker, Kubernetes, MLflow, Kafka, AWS. Leverage vector databases and streaming for real-time ML pipelines. Build generative AI/LLM features with automated evaluation pipelines. Requirements: 4+ years production ML, Python with PyTorch/LangChain/Agents, AWS/Kubernetes/ArgoCD/Docker/Terraform/Jenkins CI-CD, monitoring with Datadog/OpenSearch, FastAPI or Faust for ML services, AWS SageMaker/Bedrock, Kafka and vector databases, MLflow lifecycle management, Enterprise RAG systems (chunking, reranking). Nice to have: B2B SaaS experience, Airflow, SQL/Spark, Snowflake, agentic frameworks.",
        "fit_notes": "Strong match: Production ML experience, LLM/HuggingFace integrations from Blink, Python expert. Spain explicitly eligible. RAG systems and vector databases align with recent work. Heavy infra requirements (Kubernetes, ArgoCD) may be stretch but learnable.",
        "full_description": true
    },
    {
        "company": "Awin",
        "title": "Senior Data Scientist & ML Engineer",
        "date_found": "2026-01-06",
        "link": "https://job-boards.greenhouse.io/awin/jobs/7571203003",
        "location": "Madrid, Spain (Flexi-Office/Hybrid)",
        "description": "AI/ML workstream role designing ML applications with data scientists. Manage existing ML workloads, build batch and on-demand pipelines for AI/ML models, develop Generative AI solutions for the Agentic Era. Design scalable data pipelines for agentic and traditional ML, productionize LLM/agent-based workflows with reliability and observability, build feature stores and vector/embedding stores, implement drift detection and automated retraining. Requirements: Bachelor's/Master's in data science/CS (Master's preferred), 5+ years AI/ML data engineer, AWS/Azure, Spark/Databricks/Python, MLflow, NLP concepts, scrum/agile. Perks: Flexi-Week (4-day work week at full pay), remote allowance, Flexi-Office hybrid across regions, Awin Academy training. Mentor juniors and lead strategic AI/ML initiatives.",
        "fit_notes": "Excellent match: Madrid Flexi-Office location, production ML experience, LLM/agentic workflows, Python/cloud skills. Leadership component aligns with building teams at Blink. 4-day work week is major perk.",
        "full_description": true
    },
    {
        "company": "Bluetab (IBM Company)",
        "title": "Data Scientist",
        "date_found": "2026-01-06",
        "link": "https://www.bluetab.net/es/data-scientist/",
        "location": "Madrid, Spain (Flexible Remote)",
        "description": "IBM's data arm in Spain (Great Place to Work certified). Augmented Analytics team for national and international projects. Design full ML lifecycle, build and deploy solutions including infrastructure, data engineering, and applications. Requirements: Python, R, JavaScript, C++; ML frameworks (Scikit-Learn, TensorFlow, Keras, PyTorch, XGBoost, Horovod); MLOps and TDSP methodologies; AWS/Azure/GCP. High English valued for EMEA projects. Benefits: Indefinite contract, competitive salary, flexible telework, Madrid office (central with good transport), flexible hours, Friday/summer intensive schedule, 23 vacation days + 2 Bluetab days (Christmas Eve/New Year's Eve), restaurant card, medical/dental insurance, flexible benefits (transport/childcare), 1000+ training courses, Career Coach program. Talent hubs in Barcelona, Bizkaia, Alicante, and MÁLAGA.",
        "fit_notes": "Good match: Flexible telework in Spain with MÁLAGA talent hub explicitly mentioned. ML/cloud skills align well. IBM company stability. Training and certification opportunities. Spanish company so indefinite contract likely.",
        "full_description": true
    },
    {
        "company": "Welocalize",
        "title": "AI Machine Learning R&D Engineer",
        "date_found": "2026-01-06",
        "applied": true,
        "link": "https://jobs.lever.co/welocalize/c0df04a2-8e58-4f9b-9916-bf03fb2e87a6",
        "location": "Remote (Spain, Italy, Greece, UK, Romania, France, Portugal, Estonia, and more)",
        "description": "Welocalize is a global transformation partner delivering multilingual content for 250+ languages with 400,000+ linguistic resources. They deliver NLP-enabled ML training data solutions. Role: Design and develop ML models for localization workflows including machine translation, LLM fine-tuning, and quality assurance. Take ownership of projects from conception to deployment using AWS (SageMaker, EC2, S3, SNS), Docker, MLflow. Evaluate and optimize models using TensorFlow/PyTorch. Requirements: Master's (PhD preferred) in CS/Data Science/Engineering/Math, 3+ years ML Engineer, production-grade Python, TensorFlow/PyTorch/Scikit-learn, NLP expertise, AWS hands-on experience, GPU deployments. Flexible culture with core hours 4-6pm CET only. Success indicators: effective model development, team collaboration, continuous learning, clear communication, ethical AI.",
        "fit_notes": "Excellent fit: Remote Spain, NLP/LLM fine-tuning aligns perfectly with Blink experience, full project ownership model matches work style. PhD preferred is a plus. Machine translation domain is interesting.",
        "full_description": true,
        "cover_letter_topics": [
            {
                "topic": "Master's (PhD preferred) in CS, Data Science, Engineering, or Math",
                "relevant_experience": "PhD Mathematics (University of Reading); 3 peer-reviewed publications; statistical foundations for model development"
            },
            {
                "topic": "3+ years ML Engineer experience; production-grade Python",
                "relevant_experience": "3 years at Blink SEO building production ML platform from scratch; Python backend on GCP processing 50M+ data points daily; sklearn, Pandas, NLTK"
            },
            {
                "topic": "NLP expertise; NLP-enabled ML training data solutions",
                "relevant_experience": "Built NLP-powered features at Blink: keyword clustering using text embeddings, NLTK for text processing, semantic matching using centroid similarity"
            },
            {
                "topic": "LLM fine-tuning; evaluate and optimize models",
                "relevant_experience": "HuggingFace integrations for content generation; Ollama for local LLM deployment; prompt engineering for SEO recommendations"
            },
            {
                "topic": "Take ownership of projects from conception to deployment",
                "relevant_experience": "Owned entire Blink platform: conceived architecture, built backend, deployed to GCP, iterated based on user feedback; internal tool became standalone SaaS product"
            },
            {
                "topic": "AWS hands-on experience (SageMaker, EC2, S3)",
                "relevant_experience": "Primary cloud experience is GCP (Compute Engine, BigQuery, Cloud Functions); AWS in recent freelance work; cloud-agnostic skills transfer"
            },
            {
                "topic": "Machine translation and localization workflows",
                "relevant_experience": "NLP work with text at scale, though domain was SEO rather than translation; techniques overlap: text processing, embeddings, quality scoring"
            },
            {
                "topic": "Flexible culture with core hours 4-6pm CET only; remote-first",
                "relevant_experience": "Based in Spain (CET timezone); freelancing remotely since Jan 2025; comfortable with async collaboration"
            },
            {
                "topic": "Team collaboration; clear communication; continuous learning",
                "relevant_experience": "Worked directly with delivery team at Blink; presented at international conferences; published in peer-reviewed journals; PhD-to-industry transition shows learning adaptability"
            }
        ],
        "cover_letter": "The ownership model at Welocalize appeals to me. At Blink SEO I built an NLP-powered ML platform from scratch: I conceived the architecture, wrote the Python backend, deployed to GCP, and iterated based on user feedback until it became a standalone SaaS product. That full cycle, from conception to production system handling 50M+ data points daily, is what I do best.\n\nThe NLP work I did at Blink included keyword clustering using NLTK and text embeddings, semantic matching via centroid similarity, and HuggingFace integrations for content generation. The application was SEO rather than localization, but the techniques overlap: processing text at scale, building embeddings, scoring quality, deploying models that run reliably. I've also done prompt engineering work and local LLM deployment with Ollama.\n\nMy PhD in Mathematics gave me foundations in statistical modeling and rigorous evaluation. I've published in peer-reviewed journals and presented at international conferences, so I'm comfortable communicating technical work to different audiences.\n\nMy cloud experience is primarily GCP (Compute Engine, BigQuery, Cloud Functions) rather than AWS, though I've worked with AWS in recent freelance projects and the skills transfer. I'm based in Spain and the flexible remote setup with core hours in CET works well for me.\n\nHappy to discuss how my NLP and production ML experience fits what you're building.",
        "questions": [
            {
                "question": "What are the stages of building a model in Machine Learning?",
                "answer": "Data collection, cleaning/preprocessing, exploratory analysis, feature engineering, model selection, training, evaluation, hyperparameter tuning, deployment, monitoring."
            },
            {
                "question": "What are the differences between Deep Learning and Machine Learning.",
                "answer": "Deep learning is a subset of ML that uses neural networks with many layers. Traditional ML typically requires manual feature engineering, while DL learns feature representations directly from raw data. DL needs more data and compute but excels at unstructured data like images, text, and audio."
            },
            {
                "question": "Which built-in Python data type is immutable: list or tuple?",
                "answer": "Tuple."
            },
            {
                "question": "What file name is commonly used to list / pin Python package versions for a project?",
                "answer": "requirements.txt for pinning exact versions. pyproject.toml (PEP 621) is the modern standard for declaring dependencies, but typically with version ranges rather than pins."
            },
            {
                "question": "Which neural architecture is now the de-facto standard for state-of-the-art machine-translation models?",
                "answer": "Transformer."
            }
        ]
    },
    {
        "company": "Leadtech",
        "title": "Data Scientist (Full Remote within Spain)",
        "date_found": "2026-01-06",
        "link": "https://apply.workable.com/leadtech/j/C088ED0335/",
        "location": "100% Remote (Spain)",
        "description": "Data Scientist role at a digital innovation company with 700+ team members from 23+ nationalities. Leadtech creates innovative online solutions reaching millions of users monthly. Full remote position within Spain. NOTE: Workable uses JavaScript rendering - full job description must be verified manually on the page.",
        "fit_notes": "Location ideal: 100% remote Spain. Company appears to be digital/tech focused which aligns with background. MANUAL VERIFICATION NEEDED: Workable JS prevents scraping full requirements.",
        "full_description": false
    },
    {
        "company": "Wallapop",
        "title": "Data Scientist",
        "date_found": "2026-01-06",
        "link": "https://apply.workable.com/wallapop/j/A321A97636/",
        "location": "Barcelona, Spain",
        "description": "Data Scientist at Barcelona-based scale-up focused on conscious consumption and collaborative economy. Wallapop is a leading second-hand marketplace. NOTE: Workable uses JavaScript rendering - full job description must be verified manually on the page.",
        "fit_notes": "Barcelona location works. Marketplace/e-commerce focus similar to SEO work at Blink. Spanish company. MANUAL VERIFICATION NEEDED: Workable JS prevents scraping full requirements.",
        "full_description": false
    },
    {
        "company": "Veriff",
        "title": "Senior Data Scientist - Fraud",
        "date_found": "2026-01-07",
        "applied": true,
        "link": "https://www.veriff.com/careers/position/8302940002?gh_jid=8302940002",
        "location": "Remote (Spain, Estonia)",
        "description": "Fraud Engineering team building next-generation fraud detection platform. Responsibilities: Design and evolve fraud detection mechanisms, build real-time models to identify and block fraud, invent new detection strategies, design data-driven hypotheses to identify emerging fraud patterns, collaborate with engineers to deploy models at scale, define model monitoring strategies. Requirements: 7-8 years Data Scientist experience, production ML deployment, Python and SQL proficiency, Pandas/Polars, NumPy, Matplotlib/Seaborn, scikit-learn, XGBoost/LightGBM/CatBoost, supervised and unsupervised learning, clear communication with technical and non-technical stakeholders. Desirable: fraud detection/financial crime experience, embeddings and vector similarity search, cloud environments (AWS, GCP, Azure). Benefits: Remote or office flexibility, 4 extra recharge days, stock options, private health insurance, L&D budget.",
        "fit_notes": "Spain remote works. Strong match: Python/SQL/sklearn, production ML, embeddings/vector similarity from NLP work, cloud experience (GCP). PhD research on tipping point detection relates to anomaly/fraud detection conceptually. 7-8 years requirement is senior but PhD + Blink + NPL totals 8+ years.",
        "full_description": true,
        "cover_letter_topics": [
            {
                "topic": "7-8 years Data Scientist experience; production deployment of ML models",
                "relevant_experience": "3 years at Blink building production ML platform from scratch; 5 years at NPL doing applied research; PhD in Mathematics. Total 8+ years working with data and models."
            },
            {
                "topic": "Python and SQL proficiency; Pandas, NumPy, Matplotlib/Seaborn",
                "relevant_experience": "10+ years Python; BigQuery/PostgreSQL in production; Pandas, Matplotlib, Plotly for dashboards and analysis"
            },
            {
                "topic": "scikit-learn, XGBoost/LightGBM/CatBoost; supervised and unsupervised learning",
                "relevant_experience": "sklearn clustering (DBSCAN) for keyword grouping; classification and scoring systems; NLP with embeddings"
            },
            {
                "topic": "Embeddings and vector similarity search (desirable)",
                "relevant_experience": "Text embeddings for semantic matching; centroid similarity for product-keyword matching; HuggingFace integrations"
            },
            {
                "topic": "Cloud environments (AWS, GCP, Azure)",
                "relevant_experience": "GCP Compute Engine, BigQuery, Cloud Functions at Blink; AWS in freelance work"
            },
            {
                "topic": "Fraud detection, identifying patterns, staying ahead of sophisticated threats",
                "relevant_experience": "PhD on tipping point detection: identifying early warning signals in noisy systems before critical transitions. Conceptually similar to detecting anomalous patterns before fraud succeeds."
            },
            {
                "topic": "Collaborate with engineers to deploy and scale models; work with fraud analysts and product managers",
                "relevant_experience": "Worked directly with delivery team at Blink; built features based on user needs; comfortable with cross-functional collaboration"
            },
            {
                "topic": "Communicate findings clearly with technical and non-technical stakeholders",
                "relevant_experience": "Published in peer-reviewed journals; presented at international conferences; built dashboards for non-technical users"
            },
            {
                "topic": "Company mission: creating a safer, more secure world by establishing trust online; values trust, accountability, practical excellence",
                "relevant_experience": "Mission-driven work appeals; PhD research was about understanding critical systems; built reliable production systems that enterprise clients depended on"
            }
        ],
        "cover_letter": "Fraud detection interests me because the problem shifts: attackers adapt, so detection systems have to evolve. At Blink SEO I built a production ML platform from scratch that processed 50M+ data points daily. Python backend on GCP, sklearn clustering, text embeddings for semantic matching. I deployed models that ran reliably for enterprise clients and iterated based on what actually worked. The platform increased team productivity by 20x and became a standalone SaaS product.\n\nThe embeddings and vector similarity work I did at Blink transfers directly. I built centroid matching to connect products with high-value search terms, using weighted similarity scores. Matching new cases against known fraud patterns using vector similarity is the same technique in a different domain.\n\nMy PhD at the National Physical Laboratory focused on tipping point detection: identifying early warning signals in noisy systems before critical transitions occur. The domain was geophysical, but the core challenge is similar to catching subtle patterns before fraud succeeds. I've published in peer-reviewed journals and presented at international conferences, so I'm comfortable communicating technical work to different audiences.\n\nVeriff's mission of establishing trust online resonates with me. I'm based in Spain and the remote setup works well."
    }
]